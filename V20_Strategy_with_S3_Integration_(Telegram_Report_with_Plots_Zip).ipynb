{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulombale/swing_strategy_rahul/blob/main/Moving_Average_Strategy_with_S3_Integration_(Telegram_Report_with_Plots_Zip).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "Moving Average Strategy Daily Opportunity Scanner with AWS S3 Integration\n",
        "\n",
        "This script scans for moving average crossover opportunities, reading stock\n",
        "data and company lists from AWS S3, and saving analysis results\n",
        "back to S3, organized by date. It uses a specified AWS CLI profile for authentication.\n",
        "It generates plots for identified opportunities, uploads them individually to S3\n",
        "into source-specific folders (V40, V40Next, V200), and then compiles all plots\n",
        "into a zip file with the same folder structure to be sent via Telegram\n",
        "along with the CSV report.\n",
        "\"\"\"\n",
        "\n",
        "# --- 1. SETUP AND IMPORTS ---\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from datetime import datetime\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "import io\n",
        "import logging\n",
        "import requests\n",
        "import zipfile\n",
        "from typing import Optional, List\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- 2. CONFIGURATION ---\n",
        "\n",
        "# AWS S3 Constants\n",
        "AWS_BUCKET_NAME = 'stock-data-rahulombale'  # <--- IMPORTANT: REPLACE WITH YOUR BUCKET NAME\n",
        "AWS_REGION = 'ap-south-1'  # <--- IMPORTANT: REPLACE WITH YOUR AWS REGION\n",
        "AWS_PROFILE_NAME = 'stock_data_upload_profile' # <--- IMPORTANT: REPLACE WITH YOUR AWS CLI PROFILE NAME\n",
        "\n",
        "# S3 Base Prefixes\n",
        "S3_STOCK_DATA_PREFIX = 'stock_data/' # For historical stock data files (input)\n",
        "S3_COMPANY_LIST_PREFIX = 'company_lists/' # For company token list CSVs (input)\n",
        "S3_V20_ANALYSIS_OUTPUT_PREFIX = 'analysis/v20_strategy/' # For V20 strategy results (output)\n",
        "\n",
        "# Dynamic Date for S3 paths\n",
        "TODAY_DATE_STR = datetime.today().strftime('%Y-%m-%d') # e.g., '2025-06-27'\n",
        "TODAY_YEAR_STR = datetime.today().strftime('%Y')\n",
        "TODAY_MONTH_STR = datetime.today().strftime('%m')\n",
        "TODAY_DAY_STR = datetime.today().strftime('%d')\n",
        "\n",
        "# Full S3 paths based on today's date for output\n",
        "S3_OUTPUT_BASE_PATH = f\"{S3_V20_ANALYSIS_OUTPUT_PREFIX}{TODAY_DATE_STR}/\"\n",
        "S3_OUTPUT_CSV_KEY = f\"{S3_OUTPUT_BASE_PATH}v20_opportunities_{TODAY_DATE_STR}.csv\"\n",
        "\n",
        "# Input Data Path (assuming daily stock data is stored by date)\n",
        "S3_INPUT_STOCK_DATA_PATH = f\"{S3_STOCK_DATA_PREFIX}{TODAY_YEAR_STR}/{TODAY_MONTH_STR}/{TODAY_DAY_STR}/\"\n",
        "\n",
        "\n",
        "# Strategy Parameters\n",
        "SMA_PERIOD = 200\n",
        "MISSED_TRIGGER_THRESHOLD = 1.10 # 10% above the buy level\n",
        "TICKER_COLUMN_NAME = 'ticker'\n",
        "\n",
        "# Stock list files (read from S3_COMPANY_LIST_PREFIX)\n",
        "STOCK_LIST_FILES = [\n",
        "    'v40_token.csv',\n",
        "    'v40next_token.csv',\n",
        "    'v200_token.csv'\n",
        "]\n",
        "SOURCE_FILE_MAPPING = {\n",
        "    'v40_token.csv': 'V40',\n",
        "    'v40next_token.csv': 'V40Next',\n",
        "    'v200_token.csv': 'V200'\n",
        "}\n",
        "\n",
        "# --- Telegram Bot Configuration ---\n",
        "# IMPORTANT: Replace with your actual Bot Token and Chat ID\n",
        "TELEGRAM_BOT_TOKEN = 'YOUR_TELEGRAM_BOT_TOKEN'  # e.g., '1234567890:ABC-DEF1234ghIkl-zyx57W2v1u123ew11'\n",
        "TELEGRAM_CHAT_ID = 'YOUR_TELEGRAM_CHAT_ID'    # e.g., '-123456789' for a group, '123456789' for a user\n",
        "\n",
        "if TELEGRAM_BOT_TOKEN == 'YOUR_TELEGRAM_BOT_TOKEN' or TELEGRAM_CHAT_ID == 'YOUR_TELEGRAM_CHAT_ID':\n",
        "    logger.warning(\"WARNING: Telegram bot token or chat ID not configured. Telegram reports will not be sent.\")\n",
        "\n",
        "\n",
        "# --- 3. S3 MANAGER CLASS ---\n",
        "class S3Manager:\n",
        "    \"\"\"Manages interactions with AWS S3, using a specific profile.\"\"\"\n",
        "\n",
        "    def __init__(self, bucket_name: str, region_name: str, profile_name: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initializes the S3Manager with bucket and region, optionally using a specific AWS profile.\n",
        "\n",
        "        Args:\n",
        "            bucket_name: The name of the S3 bucket.\n",
        "            region_name: The AWS region where the bucket is located.\n",
        "            profile_name: (Optional) The name of the AWS profile to use.\n",
        "                          If None, boto3 uses its default credential chain.\n",
        "        \"\"\"\n",
        "        self.bucket_name = bucket_name\n",
        "        if profile_name:\n",
        "            session = boto3.Session(profile_name=profile_name, region_name=region_name)\n",
        "            self.s3_client = session.client('s3')\n",
        "            logger.info(f\"S3Manager initialized for bucket '{bucket_name}' in region '{region_name}' using profile '{profile_name}'.\")\n",
        "        else:\n",
        "            self.s3_client = boto3.client('s3', region_name=region_name)\n",
        "            logger.info(f\"S3Manager initialized for bucket '{bucket_name}' in region '{region_name}' using default credentials.\")\n",
        "\n",
        "    def download_csv_to_dataframe(self, s3_key: str) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Downloads a CSV file from S3 and returns it as a pandas DataFrame.\n",
        "\n",
        "        Args:\n",
        "            s3_key: The full S3 object key (path) to the CSV file.\n",
        "\n",
        "        Returns:\n",
        "            A pandas DataFrame, or None if the object does not exist or an error occurs.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)\n",
        "            df = pd.read_csv(io.BytesIO(response['Body'].read()))\n",
        "            logger.info(f\"Successfully downloaded s3://{self.bucket_name}/{s3_key}\")\n",
        "            return df\n",
        "        except ClientError as e:\n",
        "            if e.response['Error']['Code'] == 'NoSuchKey':\n",
        "                logger.warning(f\"Object not found in S3: s3://{self.bucket_name}/{s3_key}\")\n",
        "            else:\n",
        "                logger.error(f\"Error downloading from S3 ({s3_key}): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to read CSV from S3 ({s3_key}): {e}\")\n",
        "            return None\n",
        "\n",
        "    def upload_dataframe_to_s3_csv(self, df: pd.DataFrame, s3_key: str) -> None:\n",
        "        \"\"\"\n",
        "        Uploads a pandas DataFrame to S3 as a CSV file.\n",
        "\n",
        "        Args:\n",
        "            df: The pandas DataFrame to upload.\n",
        "            s3_key: The full S3 object key (path) for the CSV file.\n",
        "        \"\"\"\n",
        "        if df.empty:\n",
        "            logger.warning(f\"Attempting to upload an empty DataFrame to s3://{self.bucket_name}/{s3_key}. Skipping.\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            csv_buffer = io.StringIO()\n",
        "            df.to_csv(csv_buffer, index=False)\n",
        "            self.s3_client.put_object(Bucket=self.bucket_name, Key=s3_key, Body=csv_buffer.getvalue())\n",
        "            logger.info(f\"Successfully uploaded DataFrame to s3://{self.bucket_name}/{s3_key}\")\n",
        "        except ClientError as e:\n",
        "            logger.error(f\"Error uploading DataFrame to S3 ({s3_key}): {e}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to convert DataFrame to CSV or upload to S3 ({s3_key}): {e}\")\n",
        "            raise\n",
        "\n",
        "    def upload_image_to_s3(self, image_buffer: io.BytesIO, s3_key: str) -> None:\n",
        "        \"\"\"\n",
        "        Uploads an image (from a BytesIO buffer) to S3.\n",
        "\n",
        "        Args:\n",
        "            image_buffer: BytesIO object containing the image data.\n",
        "            s3_key: The full S3 object key (path) for the image file.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            image_buffer.seek(0) # Rewind the buffer to the beginning\n",
        "            self.s3_client.put_object(Bucket=self.bucket_name, Key=s3_key, Body=image_buffer.getvalue(), ContentType='image/png')\n",
        "            logger.info(f\"Successfully uploaded image to s3://{self.bucket_name}/{s3_key}\")\n",
        "        except ClientError as e:\n",
        "            logger.error(f\"Error uploading image to S3 ({s3_key}): {e}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to upload image to S3 ({s3_key}): {e}\")\n",
        "            raise\n",
        "\n",
        "# --- 4. CORE FUNCTIONS (MODIFIED FOR S3) ---\n",
        "\n",
        "def load_data(s3_manager: S3Manager, s3_key: str, stock_name_for_print: str) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Loads and preprocesses stock data from S3, including calculating the SMA.\n",
        "\n",
        "    Args:\n",
        "        s3_manager: An instance of S3Manager for S3 interactions.\n",
        "        s3_key: The S3 key for the stock's CSV data file.\n",
        "        stock_name_for_print: Stock name for logging/debugging.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with historical data and SMA, or None if loading fails.\n",
        "    \"\"\"\n",
        "    logger.info(f\"  - Loading data for {stock_name_for_print} from s3://{s3_manager.bucket_name}/{s3_key}\")\n",
        "    df = s3_manager.download_csv_to_dataframe(s3_key)\n",
        "\n",
        "    if df is None:\n",
        "        logger.warning(f\"  - WARNING: Data file not found or could not be loaded for {stock_name_for_print} at {s3_key}. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    if df.empty:\n",
        "        logger.warning(f\"  - WARNING: Loaded data for {stock_name_for_print} from {s3_key} is empty. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "        df.set_index('timestamp', inplace=True)\n",
        "        df.index = df.index.tz_localize(None) # Make index timezone-naive\n",
        "        df.sort_index(inplace=True)\n",
        "        df[f'SMA_{SMA_PERIOD}'] = df['close'].rolling(window=SMA_PERIOD).mean()\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"  - ERROR: Could not preprocess file for {stock_name_for_print}. Reason: {e}\")\n",
        "        return None\n",
        "\n",
        "def find_v20_setups(df: pd.DataFrame, pct_change_threshold: float = 0.20) -> List[dict]:\n",
        "    \"\"\"Identifies all qualifying v20 setups in the historical data.\"\"\"\n",
        "    setups = []\n",
        "    i = 0\n",
        "    while i < len(df):\n",
        "        if i < 1: # Cannot check previous candle if it's the first one\n",
        "            i+=1\n",
        "            continue\n",
        "\n",
        "        # A setup can only start after a red candle or at the beginning\n",
        "        is_previous_red = df['close'].iloc[i-1] <= df['open'].iloc[i-1]\n",
        "        is_current_green = df['close'].iloc[i] > df['open'].iloc[i]\n",
        "\n",
        "        if is_previous_red and is_current_green:\n",
        "            start_index = i\n",
        "            j = i\n",
        "            while j + 1 < len(df) and df['close'].iloc[j+1] > df['open'].iloc[j+1]:\n",
        "                j += 1\n",
        "            end_index = j\n",
        "\n",
        "            segment = df.iloc[start_index:end_index + 1]\n",
        "            lowest_low = segment['low'].min()\n",
        "            highest_high = segment['high'].max()\n",
        "\n",
        "            if (highest_high - lowest_low) / lowest_low > pct_change_threshold:\n",
        "                setups.append({\n",
        "                    'start_date': segment.index[0],\n",
        "                    'end_date': segment.index[-1],\n",
        "                    'lowest_low': lowest_low,\n",
        "                    'highest_high': highest_high,\n",
        "                    'scan_from_index': end_index + 1\n",
        "                })\n",
        "            i = end_index + 1\n",
        "        else:\n",
        "            i += 1\n",
        "    return setups\n",
        "\n",
        "# Global list to store plots to be zipped\n",
        "# This is used to collect all plots generated across different stock scans\n",
        "# and then zip them together at the end.\n",
        "_plots_for_zipping = []\n",
        "\n",
        "def plot_opportunity(df: pd.DataFrame, opportunity: dict, s3_manager: S3Manager, source_file_name: str) -> None:\n",
        "    \"\"\"\n",
        "    Generates a chart using Plotly and saves it to S3, and collects it for zipping.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing stock data and SMAs.\n",
        "        opportunity: Dictionary containing opportunity details (Stock, Signal Type, etc.).\n",
        "        s3_manager: An instance of S3Manager for S3 interactions.\n",
        "        source_file_name: The original company list filename (e.g., 'v40_token.csv')\n",
        "                          used to determine the source-specific folder.\n",
        "    \"\"\"\n",
        "    stock_name = opportunity['Stock']\n",
        "    signal_type = opportunity['Signal Type']\n",
        "    buy_level = opportunity['Buy Level']\n",
        "    sell_target = opportunity['Sell Target']\n",
        "    setup_start = opportunity['Setup Start']\n",
        "    setup_end = opportunity['Setup End']\n",
        "\n",
        "    # Determine the source type folder (e.g., 'V40', 'V40Next')\n",
        "    source_type_folder_name = SOURCE_FILE_MAPPING.get(source_file_name, 'UnknownSource')\n",
        "\n",
        "    # Determine plot key and box color based on signal type\n",
        "    # Construct the S3 key prefix for plots: S3_OUTPUT_BASE_PATH / buyplots / {source_type_folder} /\n",
        "    # This aligns with s3_bucket/analysis/v20_strategy/{date}/buyplots/{v40 or v40next or v200}/\n",
        "    if 'IMMEDIATE_TRIGGER' in signal_type or 'MISSED_TRIGGER_NEARBY' in signal_type:\n",
        "        metadata_text_box_color = \"#b6d8a8\" # Greenish for buy related signals\n",
        "        s3_plot_key_prefix = f\"{S3_OUTPUT_BASE_PATH}buyplots/{source_type_folder_name}/\"\n",
        "        plot_file_name = f\"{stock_name}_{signal_type.lower()}.png\" # e.g., AAPL_immediate_trigger.png\n",
        "    else: # Fallback for any other unexpected signal type\n",
        "        metadata_text_box_color = \"#cccccc\"\n",
        "        s3_plot_key_prefix = f\"{S3_OUTPUT_BASE_PATH}other_plots/{source_type_folder_name}/\" # A new folder for other types\n",
        "        plot_file_name = f\"{stock_name}_signal.png\"\n",
        "\n",
        "    s3_full_plot_key = f\"{s3_plot_key_prefix}{plot_file_name}\"\n",
        "\n",
        "    # Define the path for the plot *inside the zip file*\n",
        "    # This creates the desired nested folder structure within the zip: buyplots/{source_type_folder}/{filename}\n",
        "    plot_path_in_zip = f\"buyplots/{source_type_folder_name}/{plot_file_name}\"\n",
        "\n",
        "    # Create Plotly figure\n",
        "    fig = go.Figure(data=[\n",
        "        go.Candlestick(\n",
        "            x=df.index,\n",
        "            open=df['open'],\n",
        "            high=df['high'],\n",
        "            low=df['low'],\n",
        "            close=df['close'],\n",
        "            name='Candles'\n",
        "        ),\n",
        "        go.Scatter(x=df.index, y=df[f'SMA_{SMA_PERIOD}'], line=dict(color='blue', width=2), name=f'SMA {SMA_PERIOD}')\n",
        "    ])\n",
        "\n",
        "    # Add buy level and sell target lines\n",
        "    fig.add_shape(\n",
        "        type=\"line\",\n",
        "        x0=df.index[0], y0=buy_level,\n",
        "        x1=df.index[-1], y1=buy_level,\n",
        "        line=dict(color='green', dash='dash', width=2),\n",
        "        name='Buy Level'\n",
        "    )\n",
        "    fig.add_shape(\n",
        "        type=\"line\",\n",
        "        x0=df.index[0], y0=sell_target,\n",
        "        x1=df.index[-1], y1=sell_target,\n",
        "        line=dict(color='red', dash='dash', width=2),\n",
        "        name='Sell Target'\n",
        "    )\n",
        "\n",
        "    # Highlight the setup period\n",
        "    fig.add_vrect(\n",
        "        x0=setup_start, x1=setup_end,\n",
        "        fillcolor=\"LightSalmon\", opacity=0.2, line_width=0,\n",
        "        annotation_text=\"V20 Setup\", annotation_position=\"top left\"\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title={\n",
        "            'text': f\"{stock_name} - V20 Setup\",\n",
        "            'y':0.9, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top'\n",
        "        },\n",
        "        xaxis_title=\"Date\",\n",
        "        yaxis_title=\"Price\",\n",
        "        xaxis_rangeslider_visible=False,\n",
        "        template=\"plotly_white\",\n",
        "        height=600, width=1000 # Adjust size for plots\n",
        "    )\n",
        "\n",
        "    # Add metadata text as annotation\n",
        "    distance_pct = (opportunity['Current Close'] - opportunity['Buy Level']) / opportunity['Buy Level'] * 100\n",
        "    metadata_text = (\n",
        "        f\"Stock List: {opportunity['Source File']}<br>\"\n",
        "        f\"Signal Type: {opportunity['Signal Type']}<br>\"\n",
        "        f\"Setup Date Range: {opportunity['Setup Start']} to {opportunity['Setup End']}<br>\"\n",
        "        f\"Buy Level: {opportunity['Buy Level']:.2f}<br>\"\n",
        "        f\"Sell Target: {opportunity['Sell Target']:.2f}<br>\"\n",
        "        f\"Current Close: {opportunity['Current Close']:.2f}<br>\"\n",
        "        f\"Distance from Buy Level: {distance_pct:.2f}%<br>\"\n",
        "        f\"Potential Upside: {opportunity['Potential Upside %']:.2f}%<br>\"\n",
        "        f\"SMA Filter: {opportunity['SMA Filter Status (for V200)']}\"\n",
        "    )\n",
        "\n",
        "    fig.add_annotation(\n",
        "        xref=\"paper\", yref=\"paper\",\n",
        "        x=0.02, y=0.98,\n",
        "        text=metadata_text,\n",
        "        showarrow=False,\n",
        "        font=dict(size=12, color=\"black\"),\n",
        "        bgcolor=metadata_text_box_color,\n",
        "        bordercolor=\"black\",\n",
        "        borderwidth=1,\n",
        "        borderpad=4,\n",
        "        align=\"left\",\n",
        "        valign=\"top\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Convert Plotly figure to bytes directly (requires kaleido)\n",
        "        img_bytes = fig.to_image(format=\"png\")\n",
        "        img_buffer = io.BytesIO(img_bytes)\n",
        "\n",
        "        # Upload individual plot to S3\n",
        "        s3_manager.upload_image_to_s3(img_buffer, s3_full_plot_key)\n",
        "\n",
        "        # Add to global list for zipping later, with its path inside the zip\n",
        "        _plots_for_zipping.append((img_buffer, plot_path_in_zip))\n",
        "\n",
        "    except ImportError:\n",
        "        logger.error(\"Plotly image export requires 'kaleido'. Please install it: pip install kaleido\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Could not save or upload image for {stock_name}. Error: {e}\")\n",
        "\n",
        "\n",
        "def scan_for_opportunities(df: pd.DataFrame, stock_name: str, source_file_name: str, s3_manager: S3Manager) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Scans a single stock for current V20 trading opportunities.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing stock data.\n",
        "        stock_name: The ticker symbol of the stock.\n",
        "        source_file_name: The name of the company list file (e.g., 'v40_token.csv').\n",
        "        s3_manager: An instance of S3Manager for S3 interactions (used for plotting).\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, each representing an opportunity.\n",
        "    \"\"\"\n",
        "    opportunities = []\n",
        "\n",
        "    # We only care about the most recent valid setup\n",
        "    setups = find_v20_setups(df)\n",
        "    if not setups:\n",
        "        return []\n",
        "\n",
        "    latest_setup = setups[-1]\n",
        "    buy_level = latest_setup['lowest_low']\n",
        "    sell_target = latest_setup['highest_high']\n",
        "\n",
        "    # Analyze data from the point the setup was confirmed\n",
        "    scan_df = df.iloc[latest_setup['scan_from_index']:]\n",
        "    if scan_df.empty:\n",
        "        return []\n",
        "\n",
        "    # Check the entire history after the setup for a trigger\n",
        "    triggered_candles = scan_df[scan_df['low'] <= buy_level]\n",
        "\n",
        "    current_candle = df.iloc[-1]\n",
        "    current_close = current_candle['close']\n",
        "    current_low = current_candle['low']\n",
        "\n",
        "    metadata = {\n",
        "        'Stock': stock_name,\n",
        "        'Source File': SOURCE_FILE_MAPPING.get(source_file_name),\n",
        "        'Setup Start': latest_setup['start_date'].date(),\n",
        "        'Setup End': latest_setup['end_date'].date(),\n",
        "        'Buy Level': round(buy_level, 2),\n",
        "        'Sell Target': round(sell_target, 2),\n",
        "        'Current Close': round(current_close, 2),\n",
        "        'Distance From Buy Level %': round(((current_close - buy_level) / buy_level) * 100, 2),\n",
        "        'Potential Upside %': round(((sell_target - current_close) / current_close) * 100, 2),\n",
        "        'SMA Filter Status (for V200)': 'N/A'\n",
        "    }\n",
        "\n",
        "    # Condition 1: Immediate Trigger Today\n",
        "    if current_low <= buy_level:\n",
        "        metadata['Signal Type'] = 'IMMEDIATE_TRIGGER'\n",
        "        opportunities.append(metadata)\n",
        "\n",
        "    # Condition 2: Missed Trigger but still nearby\n",
        "    elif not triggered_candles.empty:\n",
        "        # Check if the current price is within the 10% threshold\n",
        "        if buy_level < current_close <= buy_level * MISSED_TRIGGER_THRESHOLD:\n",
        "            metadata['Signal Type'] = 'MISSED_TRIGGER_NEARBY'\n",
        "            opportunities.append(metadata)\n",
        "\n",
        "    # Apply V200 SMA Filter to any found opportunities\n",
        "    if opportunities and SOURCE_FILE_MAPPING.get(source_file_name) == 'V200':\n",
        "        sma_value = current_candle.get(f'SMA_{SMA_PERIOD}') # Use .get() to avoid KeyError if SMA not calculated\n",
        "        if sma_value is None or pd.isna(sma_value) or buy_level >= sma_value:\n",
        "            opportunities[0]['SMA Filter Status (for V200)'] = f'Not Met (Buy Level {buy_level:.2f} >= SMA {sma_value if sma_value is not None else \"N/A\":.2f})'\n",
        "        else:\n",
        "            opportunities[0]['SMA Filter Status (for V200)'] = 'Met'\n",
        "\n",
        "    # Plot only if an opportunity is found\n",
        "    if opportunities:\n",
        "        plot_opportunity(df, opportunities[0], s3_manager, source_file_name) # Pass s3_manager and source_file_name for plot organization\n",
        "\n",
        "    return opportunities\n",
        "\n",
        "def send_telegram_document(file_content: io.BytesIO, file_name: str, caption: str, content_type: str = 'text/csv') -> bool:\n",
        "    \"\"\"\n",
        "    Sends a document (e.g., CSV file, zip file) to a Telegram chat.\n",
        "\n",
        "    Args:\n",
        "        file_content: A BytesIO object containing the file's content.\n",
        "        file_name: The name of the file to send (e.g., 'report.csv').\n",
        "        caption: A text caption for the document.\n",
        "        content_type: The MIME type of the file (e.g., 'text/csv', 'application/zip').\n",
        "\n",
        "    Returns:\n",
        "        True if the document was sent successfully, False otherwise.\n",
        "    \"\"\"\n",
        "    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID or TELEGRAM_BOT_TOKEN == 'YOUR_TELEGRAM_BOT_TOKEN':\n",
        "        logger.error(\"Telegram bot token or chat ID not configured. Skipping Telegram document send.\")\n",
        "        return False\n",
        "\n",
        "    url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendDocument\"\n",
        "\n",
        "    files = {\n",
        "        'document': (file_name, file_content.getvalue(), content_type)\n",
        "    }\n",
        "    data = {\n",
        "        'chat_id': TELEGRAM_CHAT_ID,\n",
        "        'caption': caption\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, data=data, files=files)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        logger.info(f\"Telegram document '{file_name}' sent successfully.\")\n",
        "        return True\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Failed to send Telegram document '{file_name}': {e}\")\n",
        "        if hasattr(e, 'response') and e.response is not None:\n",
        "            logger.error(f\"Telegram API Response: {e.response.text}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred while sending Telegram document: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "# --- 5. MAIN EXECUTION BLOCK ---\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Clear the global list of plots at the beginning of execution\n",
        "    _plots_for_zipping = []\n",
        "\n",
        "    all_opportunities = []\n",
        "\n",
        "    # Initialize S3Manager with the specified profile\n",
        "    s3_manager = S3Manager(AWS_BUCKET_NAME, AWS_REGION, AWS_PROFILE_NAME)\n",
        "\n",
        "    logger.info(\"--- Starting V20 Daily Opportunity Scanner ---\")\n",
        "    logger.info(f\"Reading input data from S3 path: s3://{AWS_BUCKET_NAME}/{S3_INPUT_STOCK_DATA_PATH}\")\n",
        "    logger.info(f\"Saving results to S3 path: s3://{AWS_BUCKET_NAME}/{S3_OUTPUT_BASE_PATH}\")\n",
        "\n",
        "    # Consolidate all tickers from all files into one list (read from S3)\n",
        "    all_tickers_to_scan = {} # Using dict to store ticker and its source file\n",
        "    for list_file_name in STOCK_LIST_FILES:\n",
        "        stock_list_s3_key = f\"{S3_COMPANY_LIST_PREFIX}{list_file_name}\"\n",
        "        logger.info(f\"  - Attempting to load stock list from s3://{AWS_BUCKET_NAME}/{stock_list_s3_key}\")\n",
        "\n",
        "        stocks_df = s3_manager.download_csv_to_dataframe(stock_list_s3_key)\n",
        "\n",
        "        if stocks_df is None or stocks_df.empty:\n",
        "            logger.warning(f\"  - WARNING: Stock list file not found or empty at s3://{AWS_BUCKET_NAME}/{stock_list_s3_key}. Skipping this list.\")\n",
        "            continue # Skip to the next list file if this one fails\n",
        "\n",
        "        for ticker in stocks_df[TICKER_COLUMN_NAME].dropna().unique():\n",
        "            if ticker not in all_tickers_to_scan:\n",
        "                all_tickers_to_scan[ticker] = list_file_name\n",
        "\n",
        "    if not all_tickers_to_scan:\n",
        "        logger.fatal(\"FATAL ERROR: No unique stocks found to scan from any company list. Exiting.\")\n",
        "        exit(1) # Exit if no essential stock list can be loaded\n",
        "\n",
        "    logger.info(f\"Found a total of {len(all_tickers_to_scan)} unique stocks to scan.\")\n",
        "\n",
        "    # Loop through the consolidated list of tickers\n",
        "    for i, (ticker, source_file) in enumerate(all_tickers_to_scan.items()):\n",
        "        logger.info(f\"  [{i+1}/{len(all_tickers_to_scan)}] Analyzing: {ticker}\")\n",
        "\n",
        "        # Construct S3 key for individual stock data\n",
        "        data_s3_key = f\"{S3_INPUT_STOCK_DATA_PATH}{ticker}.csv\"\n",
        "        df = load_data(s3_manager, data_s3_key, ticker)\n",
        "\n",
        "        if df is not None and not df.empty:\n",
        "            # Pass s3_manager AND source_file_name to scan_for_opportunities\n",
        "            opportunities = scan_for_opportunities(df, ticker, source_file, s3_manager)\n",
        "            if opportunities:\n",
        "                all_opportunities.extend(opportunities)\n",
        "\n",
        "    logger.info(\"\\n--- Scan Complete ---\")\n",
        "\n",
        "    # --- 6. FINAL REPORTING (MODIFIED FOR S3 & TELEGRAM) ---\n",
        "    if all_opportunities:\n",
        "        report_df = pd.DataFrame(all_opportunities)\n",
        "        # Reorder columns for better readability\n",
        "        cols_order = ['Stock', 'Source File', 'Signal Type', 'Current Close',\n",
        "                      'Buy Level', 'Distance From Buy Level %',\n",
        "                      'Sell Target', 'Potential Upside %', 'SMA Filter Status (for V200)',\n",
        "                      'Setup Start', 'Setup End']\n",
        "        report_df = report_df.reindex(columns=cols_order)\n",
        "\n",
        "        # Sort by Signal Type and Distance for priority\n",
        "        report_df['sort_key'] = report_df['Signal Type'].apply(lambda x: {'IMMEDIATE_TRIGGER':0, 'MISSED_TRIGGER_NEARBY':1}.get(x, 99))\n",
        "        report_df = report_df.sort_values(by=['sort_key', 'Distance From Buy Level %'], ascending=[False, True])\n",
        "        report_df = report_df.drop(columns=['sort_key'])\n",
        "\n",
        "        # Save report CSV to S3\n",
        "        s3_manager.upload_dataframe_to_s3_csv(report_df, S3_OUTPUT_CSV_KEY)\n",
        "\n",
        "        logger.info(f\"\\nSUCCESS: Found {len(report_df)} opportunities.\")\n",
        "        logger.info(\"Detailed report saved to S3 at:\")\n",
        "        logger.info(f\"s3://{AWS_BUCKET_NAME}/{S3_OUTPUT_BASE_PATH}\")\n",
        "        logger.info(\"\\n\" + \"=\"*28 + \" OPPORTUNITY SUMMARY \" + \"=\"*28)\n",
        "        logger.info(\"\\n\" + report_df.to_string())\n",
        "\n",
        "        # --- Send CSV report via Telegram ---\n",
        "        telegram_csv_buffer = io.BytesIO()\n",
        "        report_df.to_csv(telegram_csv_buffer, index=False)\n",
        "        telegram_csv_buffer.seek(0) # Rewind the buffer\n",
        "\n",
        "        csv_caption = f\"V20 Daily Opportunity Report for {TODAY_DATE_STR}\\n\\n\" \\\n",
        "                      f\"Found {len(report_df)} opportunities.\\n\" \\\n",
        "                      f\"See attached CSV for full details.\"\n",
        "\n",
        "        send_telegram_document(\n",
        "            file_content=telegram_csv_buffer,\n",
        "            file_name=f\"v20_opportunities_{TODAY_DATE_STR}.csv\",\n",
        "            caption=csv_caption,\n",
        "            content_type='text/csv'\n",
        "        )\n",
        "\n",
        "        # --- Send plots as a zip file via Telegram ---\n",
        "        if _plots_for_zipping:\n",
        "            zip_buffer = io.BytesIO()\n",
        "            with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "                for plot_img_buffer, plot_path_in_zip in _plots_for_zipping:\n",
        "                    # Rewind the image buffer before writing to zip\n",
        "                    plot_img_buffer.seek(0)\n",
        "                    # Add the plot to the zip file with its intended internal path\n",
        "                    zf.writestr(plot_path_in_zip, plot_img_buffer.getvalue())\n",
        "            zip_buffer.seek(0) # Rewind the zip buffer\n",
        "\n",
        "            zip_file_name = f\"v20_plots_{TODAY_DATE_STR}.zip\"\n",
        "            zip_caption = f\"V20 Daily Opportunity Plots for {TODAY_DATE_STR}\\n\\n\" \\\n",
        "                          f\"Attached are {len(_plots_for_zipping)} plots for the identified opportunities, organized by source list.\"\n",
        "\n",
        "            send_telegram_document(\n",
        "                file_content=zip_buffer,\n",
        "                file_name=zip_file_name,\n",
        "                caption=zip_caption,\n",
        "                content_type='application/zip'\n",
        "            )\n",
        "        else:\n",
        "            logger.info(\"No plots were generated to send via Telegram.\")\n",
        "\n",
        "    else:\n",
        "        logger.info(\"\\nNo V20 opportunities found for the given date.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8xqEqHpQd8be"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
